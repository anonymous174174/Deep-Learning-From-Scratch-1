{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbefc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define directory structure\n",
    "dirs = [\n",
    "    \"data\",\n",
    "    \"models\",\n",
    "    \"optim\",\n",
    "    \"loss\",\n",
    "    \"utils\",\n",
    "    \"experiments\"\n",
    "]\n",
    "\n",
    "# Define empty files to create\n",
    "files = [\n",
    "    \"train.py\",\n",
    "    \"config.py\",\n",
    "    \"requirements.txt\",\n",
    "    \"README.md\",\n",
    "    \"data/dataloader.py\",\n",
    "    \"models/layers.py\",\n",
    "    \"models/neuralnet.py\",\n",
    "    \"optim/optimizers.py\",\n",
    "    \"loss/loss_functions.py\",\n",
    "    \"utils/metrics.py\",\n",
    "    \"utils/wandb_utils.py\",\n",
    "    \"experiments/sweep_config.yaml\"\n",
    "]\n",
    "\n",
    "# Create directories\n",
    "for dir_path in dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Create empty files\n",
    "for file_path in files:\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"# \" + os.path.basename(file_path))\n",
    "\n",
    "print(\"✅ Project structure created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce9079",
   "metadata": {},
   "source": [
    "conda command\n",
    "conda create -n dl_cpu_env python=3.10 pytorch torchvision torchaudio cpuonly numpy pandas -c pytorch -c conda-forge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a200e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "w=torch.randn((30,3))\n",
    "x=torch.randn((3,))\n",
    "b=torch.randn((30,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape,w.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(w@x+b,x@w.T+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72558d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# suppose\n",
    "# A has shape (B, k2)\n",
    "# B has shape (B, k1)\n",
    "B = 5\n",
    "k1 = 3\n",
    "k2 = 4\n",
    "A = torch.randn(B, k2)\n",
    "B = torch.randn(B, k1)\n",
    "\n",
    "# step 1: make them broadcastable to (B, k2, k1)\n",
    "#   A.unsqueeze(2) has shape (B, k2, 1)\n",
    "#   B.unsqueeze(1) has shape (B, 1, k1)\n",
    "outer = A.unsqueeze(2) * B.unsqueeze(1)\n",
    "# outer.shape == (B, k2, k1)\n",
    "\n",
    "# step 2: sum over the batch dimension → (k2, k1), then add the leading dim\n",
    "final = outer.sum(dim=0, keepdim=True)\n",
    "# final.shape == (1, k2, k1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5588df",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270cdcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b816e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,:].T.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ac406",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,:].T@B[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93508190",
   "metadata": {},
   "outputs": [],
   "source": [
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,:].T.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc28ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "B[0,:].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b04428",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,:].T.unsqueeze(1)@B[0,:].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# B = 5\n",
    "# k1 = 3\n",
    "# k2 = 4\n",
    "# A = torch.randn(B, k2)\n",
    "# B = torch.randn(B, k1)\n",
    "\n",
    "# direct outer‐product per batch\n",
    "outer = torch.einsum('bi,bj->bij', A, B)\n",
    "# outer.shape == (B, k2, k1)\n",
    "\n",
    "# sum over batch\n",
    "final = outer.sum(0, keepdim=True)\n",
    "# final.shape == (1, k2, k1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=range(5,0,-1)\n",
    "print(type(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,-1,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e594bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import torch.nn as nn\n",
    "# Adjust path so you can import your models\n",
    "# sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from models.backprop import Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a77fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBackpropagation:\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(42)\n",
    "        self.bp = Backpropagation()\n",
    "        self.batch_size = 5\n",
    "        self.curr_neurons = 3\n",
    "        self.prev_neurons = 4\n",
    "        \n",
    "        self.predictions = torch.softmax(torch.randn(self.batch_size, self.curr_neurons), dim=1)\n",
    "        indices = torch.randint(0, self.curr_neurons, (self.batch_size,))\n",
    "        self.targets = torch.zeros_like(self.predictions)\n",
    "        self.targets[torch.arange(self.batch_size), indices] = 1\n",
    "        \n",
    "        self.weight_matrix = torch.randn(self.curr_neurons, self.prev_neurons)\n",
    "        self.gradient_pre_activation = torch.randn(self.batch_size, self.curr_neurons)\n",
    "        self.activations_prev = torch.randn(self.batch_size, self.prev_neurons)\n",
    "        self.activation = torch.sigmoid(torch.randn(self.batch_size, self.curr_neurons))\n",
    "    \n",
    "    def test_cross_entropy_output_grad(self):\n",
    "        expected = -(self.targets - self.predictions)/ self.batch_size\n",
    "        result = self.bp.cross_entropy_output_grad(self.predictions, self.targets)\n",
    "        print(\"cross_entropy_output_grad:\", torch.allclose(result, expected, atol=1e-6))\n",
    "    \n",
    "    def test_dloss_dactivations_prev(self):\n",
    "        expected = self.gradient_pre_activation @ self.weight_matrix\n",
    "        result = self.bp.dloss_dactivations_prev(self.weight_matrix, self.gradient_pre_activation)\n",
    "        print(\"dloss_dactivations_prev:\", torch.allclose(result, expected, atol=1e-6))\n",
    "    \n",
    "    def test_dloss_dweights(self):\n",
    "        expected = self.gradient_pre_activation.T @ self.activations_prev\n",
    "        result = self.bp.dloss_dweights(self.activations_prev, self.gradient_pre_activation)\n",
    "        print(\"dloss_dweights:\", torch.allclose(result, expected, atol=1e-6))\n",
    "    \n",
    "    def test_dloss_dbias(self):\n",
    "        expected = self.gradient_pre_activation.sum(dim=0)\n",
    "        result = self.bp.dloss_dbias(self.gradient_pre_activation)\n",
    "        print(\"dloss_dbias:\", torch.allclose(result, expected, atol=1e-6))\n",
    "    \n",
    "    def test_dactivation_dpreactivation_sigmoid(self):\n",
    "        expected = self.activation * (1 - self.activation)\n",
    "        result = self.bp.dactivation_dpreactivation(self.activation, 'sigmoid')\n",
    "        print(\"dactivation_dpreactivation (sigmoid):\", torch.allclose(result, expected, atol=1e-6))\n",
    "    \n",
    "    def test_dactivation_dpreactivation_tanh(self):\n",
    "        act = torch.tanh(torch.randn(self.batch_size, self.curr_neurons))\n",
    "        expected = 1 - act.pow(2)\n",
    "        result = self.bp.dactivation_dpreactivation(act, 'tanh')\n",
    "        print(\"dactivation_dpreactivation (tanh):\", torch.allclose(result, expected, atol=1e-6))\n",
    "    \n",
    "    def test_dactivation_dpreactivation_relu(self):\n",
    "        act = torch.randn(self.batch_size, self.curr_neurons)\n",
    "        expected = (act > 0).to(act.dtype)\n",
    "        result = self.bp.dactivation_dpreactivation(act, 'relu')\n",
    "        print(\"dactivation_dpreactivation (relu):\", torch.equal(result, expected))\n",
    "    \n",
    "    def test_gradients_model_single_layer(self):\n",
    "        class DummyLayer:\n",
    "            def __init__(self, weight, bias, input):\n",
    "                self.weight = weight\n",
    "                self.bias = bias\n",
    "                self.input = input\n",
    "        \n",
    "        dummy_weight = torch.randn(self.curr_neurons, self.prev_neurons, requires_grad=True)\n",
    "        dummy_bias = torch.randn(self.curr_neurons, requires_grad=True)\n",
    "        dummy_input = torch.randn(self.batch_size, self.prev_neurons)\n",
    "        layer = DummyLayer(weight=dummy_weight, bias=dummy_bias, input=dummy_input)\n",
    "        \n",
    "        predictions = dummy_input @ dummy_weight.t() + dummy_bias \n",
    "        predictions = torch.softmax(predictions, dim=1)\n",
    "        indices = torch.randint(0, self.curr_neurons, (self.batch_size,))\n",
    "        targets = torch.zeros_like(predictions)\n",
    "        targets[torch.arange(self.batch_size), indices] = 1\n",
    "        \n",
    "        grad_output = self.bp.cross_entropy_output_grad(predictions, targets)\n",
    "        layer.weight.grad = self.bp.dloss_dweights(dummy_input, grad_output)\n",
    "        layer.bias.grad = self.bp.dloss_dbias(grad_output)\n",
    "        \n",
    "        dummy_weight2 = dummy_weight.clone().detach().requires_grad_(True)\n",
    "        dummy_bias2 = dummy_bias.clone().detach().requires_grad_(True)\n",
    "        predictions2 = dummy_input @ dummy_weight2.t() + dummy_bias2\n",
    "        predictions2 = torch.softmax(predictions2, dim=1)\n",
    "        log_preds = torch.log(predictions2 + 1e-9)\n",
    "        loss = -(targets * log_preds).sum()/self.batch_size\n",
    "        print(loss)\n",
    "        #loss=loss.sum(dim=0)/self.batch_size\n",
    "        loss.backward()\n",
    "        \n",
    "        print(\"gradients_model: weight grad:\", torch.allclose(layer.weight.grad, dummy_weight2.grad, atol=1e-6))\n",
    "        print(\"gradients_model: bias grad:\", torch.allclose(layer.bias.grad, dummy_bias2.grad, atol=1e-6))\n",
    "        print(\"pytorch: weight grad:\", dummy_weight2.grad)\n",
    "        print(\"pytorch: bias grad:\", dummy_bias2.grad)\n",
    "        print(\"custom: weight grad:\", layer.weight.grad)\n",
    "        print(\"custom: bias grad:\", layer.bias.grad)\n",
    "\n",
    "    def test_gradients_model_multi_layer(self):\n",
    "        # Create a 2-layer model (input_size=4, hidden_size=5, output_size=3)\n",
    "        input_size = 4\n",
    "        hidden_size = 5\n",
    "        output_size = 3\n",
    "        batch_size = 5\n",
    "        \n",
    "        class DummyLayer:\n",
    "            def __init__(self, weight, bias):\n",
    "                self.weight = weight\n",
    "                self.bias = bias\n",
    "                self.input = None  # Previous layer's post-activation output\n",
    "                self.pre_activation = None  # Current layer's pre-activation\n",
    "\n",
    "            def forward(self, x):\n",
    "                self.input = x.detach()  # Store input (a_{l-1})\n",
    "                self.pre_activation = x @ self.weight.T + self.bias  # Store z_l\n",
    "                return self.pre_activation\n",
    "\n",
    "        # Initialize custom model\n",
    "        W1 = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "        b1 = torch.randn(hidden_size, requires_grad=True)\n",
    "        W2 = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "        b2 = torch.randn(output_size, requires_grad=True)\n",
    "        \n",
    "        layer1 = DummyLayer(W1, b1)\n",
    "        layer2 = DummyLayer(W2, b2)\n",
    "        custom_model = [layer1, layer2]\n",
    "        \n",
    "        # Forward pass with custom model\n",
    "        x = torch.randn(batch_size, input_size)\n",
    "        z1 = layer1.forward(x)\n",
    "        a1 = torch.relu(z1)\n",
    "        z2 = layer2.forward(a1)\n",
    "        predictions = torch.softmax(z2, dim=1)\n",
    "        \n",
    "        # Create targets\n",
    "        targets = torch.zeros_like(predictions)\n",
    "        targets[torch.arange(batch_size), torch.randint(0, output_size, (batch_size,))] = 1\n",
    "        \n",
    "        # Compute custom gradients\n",
    "        bp = Backpropagation()\n",
    "        bp.gradients_model(custom_model, predictions, targets, 'relu')\n",
    "        \n",
    "        # Compute PyTorch gradients\n",
    "        torch_model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            torch_model[0].weight.copy_(W1)\n",
    "            torch_model[0].bias.copy_(b1)\n",
    "            \n",
    "            torch_model[2].weight.copy_(W2)\n",
    "            torch_model[2].bias.copy_(b2)\n",
    "        \n",
    "        torch_model.zero_grad()\n",
    "\n",
    "\n",
    "        \n",
    "        torch_pred = torch_model(x)\n",
    "        loss = nn.CrossEntropyLoss()(torch_pred, targets.argmax(dim=1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # Compare gradients\n",
    "        print(\n",
    "                        \"Input layer weight gradients:\",torch.allclose(custom_model[0].weight.grad, torch_model[0].weight.grad, atol=1e-6))\n",
    "        print(\n",
    "                        \"Input layer bias gradients:\",torch.allclose(custom_model[0].bias.grad, torch_model[0].bias.grad, atol=1e-6))\n",
    "        print(\"Output layer weight gradients:\",torch.allclose(custom_model[1].weight.grad, torch_model[2].weight.grad, atol=1e-6)\n",
    "                        )\n",
    "        print(\"Output layer bias gradients:\",torch.allclose(custom_model[1].bias.grad, torch_model[2].bias.grad, atol=1e-6)\n",
    "                        )\n",
    "        print(\"Custom gradients:\")\n",
    "        print(\"Input layer weight grad:\", custom_model[0].weight.grad)\n",
    "        print(\"Input layer bias grad:\", custom_model[0].bias.grad)\n",
    "        print(\"Output layer weight grad:\", custom_model[1].weight.grad)\n",
    "        print(\"Output layer bias grad:\", custom_model[1].bias.grad)\n",
    "        print(\"PyTorch gradients:\")\n",
    "        print(\"Input layer weight grad:\", torch_model[0].weight.grad)\n",
    "        print(\"Input layer bias grad:\", torch_model[0].bias.grad)\n",
    "        print(\"Output layer weight grad:\", torch_model[2].weight.grad)\n",
    "        print(\"Output layer bias grad:\", torch_model[2].bias.grad)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afd188e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = TestBackpropagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8186c678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_entropy_output_grad: True\n",
      "dloss_dactivations_prev: True\n",
      "dloss_dweights: True\n",
      "dloss_dbias: True\n",
      "dactivation_dpreactivation (sigmoid): True\n",
      "dactivation_dpreactivation (tanh): True\n",
      "dactivation_dpreactivation (relu): True\n"
     ]
    }
   ],
   "source": [
    "tester.test_cross_entropy_output_grad()\n",
    "tester.test_dloss_dactivations_prev()\n",
    "tester.test_dloss_dweights()\n",
    "tester.test_dloss_dbias()\n",
    "tester.test_dactivation_dpreactivation_sigmoid()\n",
    "tester.test_dactivation_dpreactivation_tanh()\n",
    "tester.test_dactivation_dpreactivation_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa33042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0822, grad_fn=<DivBackward0>)\n",
      "gradients_model: weight grad: True\n",
      "gradients_model: bias grad: True\n",
      "pytorch: weight grad: tensor([[ 0.3536,  0.0271, -0.0773, -0.0675],\n",
      "        [-0.2117,  0.0612,  0.1075,  0.0271],\n",
      "        [-0.1419, -0.0884, -0.0302,  0.0404]])\n",
      "pytorch: bias grad: tensor([-0.1612,  0.1934, -0.0322])\n",
      "custom: weight grad: tensor([[ 0.3536,  0.0271, -0.0773, -0.0675],\n",
      "        [-0.2117,  0.0612,  0.1075,  0.0271],\n",
      "        [-0.1419, -0.0884, -0.0302,  0.0404]], grad_fn=<MmBackward0>)\n",
      "custom: bias grad: tensor([-0.1612,  0.1934, -0.0322], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "tester.test_gradients_model_single_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d690ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer weight gradients: True\n",
      "Input layer bias gradients: True\n",
      "Output layer weight gradients: True\n",
      "Output layer bias gradients: True\n",
      "Custom gradients:\n",
      "Input layer weight grad: tensor([[-0.1258, -0.0947,  0.0406, -0.0294],\n",
      "        [ 0.0321,  0.0304,  0.0228,  0.0113],\n",
      "        [ 0.0314, -0.0270, -0.0026, -0.0101],\n",
      "        [-0.4770, -0.4511, -0.3381, -0.1679],\n",
      "        [ 0.1588,  0.1597,  0.0762,  0.0669]], grad_fn=<MmBackward0>)\n",
      "Input layer bias grad: tensor([-0.0627, -0.0229, -0.0280,  0.3399, -0.0931], grad_fn=<SumBackward1>)\n",
      "Output layer weight grad: tensor([[ 0.0559, -0.0167,  0.2875, -0.2814, -0.5276],\n",
      "        [ 0.0347,  0.0023, -0.0508,  0.0393,  0.3195],\n",
      "        [-0.0906,  0.0144, -0.2368,  0.2420,  0.2081]], grad_fn=<MmBackward0>)\n",
      "Output layer bias grad: tensor([-0.1377,  0.0982,  0.0395], grad_fn=<SumBackward1>)\n",
      "PyTorch gradients:\n",
      "Input layer weight grad: tensor([[-0.1258, -0.0947,  0.0406, -0.0294],\n",
      "        [ 0.0321,  0.0304,  0.0228,  0.0113],\n",
      "        [ 0.0314, -0.0270, -0.0026, -0.0101],\n",
      "        [-0.4770, -0.4511, -0.3381, -0.1679],\n",
      "        [ 0.1588,  0.1597,  0.0762,  0.0669]])\n",
      "Input layer bias grad: tensor([-0.0627, -0.0229, -0.0280,  0.3399, -0.0931])\n",
      "Output layer weight grad: tensor([[ 0.0559, -0.0167,  0.2875, -0.2814, -0.5276],\n",
      "        [ 0.0347,  0.0023, -0.0508,  0.0393,  0.3195],\n",
      "        [-0.0906,  0.0144, -0.2368,  0.2420,  0.2081]])\n",
      "Output layer bias grad: tensor([-0.1377,  0.0982,  0.0395])\n"
     ]
    }
   ],
   "source": [
    "tester.test_gradients_model_multi_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ae2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "bp = Backpropagation()\n",
    "batch_size = 5\n",
    "curr_neurons = 3\n",
    "prev_neurons = 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4603bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_weight = torch.randn(curr_neurons, prev_neurons, requires_grad=True)\n",
    "dummy_bias = torch.randn(curr_neurons, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3f188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(batch_size, prev_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ed2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=torch.softmax(dummy_input @ dummy_weight.t() + dummy_bias,dim=1 )\n",
    "#predictions = torch.softmax(torch.randn(batch_size, curr_neurons), dim=1)\n",
    "indices = torch.randint(0, curr_neurons, (batch_size,))\n",
    "targets = torch.zeros_like(predictions)\n",
    "targets[torch.arange(batch_size), indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d04ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(predictions+1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe820a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "-(targets * torch.log(predictions + 1e-9))#.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6682612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=-(targets * torch.log(predictions + 1e-9)).sum(dim=0).sum(dim=0)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a6e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "178fb855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5237,  0.1172, -0.2873,  0.1701],\n",
       "        [ 0.4422, -0.3693,  0.7290, -0.2545],\n",
       "        [ 0.0815,  0.2521, -0.4417,  0.0844]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_weight.grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2306de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4904, -0.4012, -0.0892])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d1a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_loss_pre_activation_current_layer =bp.cross_entropy_output_grad(predictions, targets)/batch_size\n",
    "custom_weight_grad = bp.dloss_dweights(dummy_input, gradient_loss_pre_activation_current_layer)\n",
    "custom_bias_grad = bp.dloss_dbias(gradient_loss_pre_activation_current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1cfbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5237,  0.1172, -0.2873,  0.1701],\n",
       "        [ 0.4422, -0.3693,  0.7290, -0.2545],\n",
       "        [ 0.0815,  0.2521, -0.4417,  0.0844]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbecc401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4904, -0.4012, -0.0892], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e51f7b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2019,  1.5077,  0.3483,  0.3551],\n",
       "        [ 0.8158, -0.3620,  0.0705,  0.6056],\n",
       "        [ 0.2442,  0.3988,  1.7937,  0.4370]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.randn((3,4))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a166f91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1996,  0.1070, -2.6345, -0.1302],\n",
       "        [-0.2054,  0.3407, -0.0152,  0.1570],\n",
       "        [-0.1151, -0.2015, -0.1506, -0.1012]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=torch.randn((3,4))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b222f9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0403,  0.1613, -0.9175, -0.0462],\n",
       "        [-0.1675, -0.1233, -0.0011,  0.0951],\n",
       "        [-0.0281, -0.0803, -0.2702, -0.0442]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c85e920e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0403)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0,0]*B[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2020d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=torch.randn((3,4))\n",
    "t.grad=torch.randn((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8295c20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2357, -0.2918,  0.6404, -0.1239],\n",
       "        [ 0.1319,  0.9267, -1.2518, -1.3380],\n",
       "        [-0.1074, -0.6182,  1.9594, -1.7176]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfada242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_cpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
